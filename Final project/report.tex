\documentclass[11pt,a4paper]{article}

\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[french]{babel}


\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-plot}
\usepackage{calc}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}  
\usepackage{lmodern}
\usepackage{stmaryrd}
\usepackage[]{algorithm2e}
\usepackage{float}

\pagestyle{plain}

\title{ADVANCED LEARNING FOR TEXT AND GRAPH DATA \\ Final Project: Text categorization}
\author{Mathurin \textsc{Massias} \and Clément \textsc{Nicolle} \and Michaël Weiss}
\date{\today} 


\begin{document}
	
\maketitle

\section{Bag-of-words model}

\subsection{Description and classification algorithms}

We first implemented the bag-of-words model. From the train and test datasets, two matrices of size $n\_documents \times n\_terms$ were computed, using the dictionary of terms present in the train dataset. Then, each document is represented by a vector in dimension $n\_terms$ (a line of the matrix).
\\The training set contains 14,575 different words in 5485 documents. The test set is composed of 2,189 documents. The features are extracted in 60 s.
\\We tried out several classification algorithms on the feature dataset:
\begin{itemize}
	\item k-Nearest Neighbors: for a given document of the test dataset, we find out the k closest documents in the train dataset using cosine similarity. Then, we assign to the test document the most frequent label among these k neighbors. Arbitrarily, in case of tie, the first label in alphabetic order among the most frequent will be picked up.
	\item Support Vector Machine: we used a Gaussian kernel, and cross-validation was made in order to select best values for parameters C and gamma.
	\item Random Forest: we made a cross-validation to select the optimal number of trees to compute.
	\item Adaboost: we also made a cross-validation for the number of boundaries here.
\end{itemize}


\subsection{Results}

As suggested in the subject, we used micro-averaging and macro-averaging precision and recall in order to evaluate the performances of our classifiers. We can notice that micro-averaging precision and recall are indeed the same number.
\\Cross-validations were made upstream in order to find the optimal values for the parameters: k for k-NN, C and $\gamma$ for RBF-SVM, number of trees for random forests. With these parameters fitted, here are the different performances the algorithms achieved:

\begin{table}[h]
\hspace*{-17mm}	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Algorithm \textbackslash Performance} & \begin{tabular}[c]{@{}c@{}}Micro-averaging\\ precision/recall\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ precision\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ recall\end{tabular} & Training time \\ \hline
		k-NN                                        & 84.42\%                                                                    & 85.40\%                                                             & 82.74\%                                                          & 1543 s           \\ \hline
		SVM                                         & 89.31\%                                                                    & 92.63\%                                                             & 65.06\%                                                          & 1021 s           \\ \hline
		Random Forest                               & 91.6\%                                                                     & 89.46\%                                                             & 62.84\%                                                          & 7 s           \\ \hline
		Adaboost                                    & 79.1\%                                                                    & 57.02\%                                                             & 57.86\%                                                          & 204 s           \\ \hline
	\end{tabular}
	\caption{Time and classification performance for various algorithms on the bag-of-words model}
	\end{table}
For algorithms with a part of randomness (RF), the results are averaged over 10 repetitions.
\\[5mm]The second quickest, Adaboost performs poorly compared to other algorithms. 
All other algorithms have very good classification performance. SVM takes time to train, because we use a "one-vs-one" multiclass SVM (needing 14*13/2 binary SVM training). k-NN does not require training, but each classification requires computation of 5,485 cosine distances in dimension 14,575, which takes approximately one second.
\\Though slowly inferior in terms of macro averaging recall, Random Forest is by far the fastest algorithm (150 times quicker!).
\\Depending on the trade-off between training time and recall, we would recommend using either a one-vs-one multiclass SVM or a Random Forest for the bag-of-words model.

\section{Graph-of-words model}
The we implemented a Graph-of-words model. As the subject suggested it we firstly tried to implement it with windows of size $4$ to find the edges. This allowed us to have better classification results overall. \\
The two measure we associated to the graph nodes were degree centrality measure and eigenvector centrality measure.
The two models of graphs we used were undirected unweigthed graphs, and undirected unweighted multigraphs of the library networkx. The second one allows us to have multiple same edge between $2$ nodes and therefore to have an equivalent of an undirected weighted graph.\\
Since after the graphs were constructed, the final input to use for the classification algorithms remained a document-term matrix of the same size than the one obtained in the bag-of-words model, we applied the same classification algorithms in order to compare the results.\\
The same way the score of words per document were penalized by the factor $IDF$ in the bag-of-words model, we also penalized the document-term matrix by the $IDF$ factors.
\subsection{Graphs with unweighted edges}
\paragraph{Degree Centrality measure \newline}
The time of execution needed to compute the document term matrix (and the graphs for every document before) was approximately of 300s. This time was fast enough to allows us to test several different parameters to try to find the best approach for our classification.

\begin{table}[H]
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Algorithm \textbackslash Performance} & \begin{tabular}[c]{@{}c@{}}Micro-averaging\\ precision/recall\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ precision\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ recall\end{tabular} \\ \hline
		k-NN                                        & 86.16\%                                                                    & 83.64\%                                                             & 82.22\%           \\ \hline
		SVM                                         & 95.16\%                                                                    & 95.44\%                                                             & 79.89\%            \\ \hline
		Random Forest                               & 91.09\%                                                                     & 86.05\%                                                             & 66.07\%             \\ \hline
		Adaboost                                    & 79.53\%                                                                    & 67.18\%                                                             & 67.36\%              \\ \hline
	\end{tabular}
	\caption{Classification performance by algorithm for Degree Centrality measure}
\end{table}

\paragraph{Eigenvector Centrality measure \newline}
Here, using the networkx eigenvector centrality measure the time of execution needed to compute the document term matrix was around $450$s.  
\begin{table}[H]
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Algorithm \textbackslash Performance} & \begin{tabular}[c]{@{}c@{}}Micro-averaging\\ precision/recall\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ precision\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ recall\end{tabular} \\ \hline
		k-NN                                        & 86.71\%                                                                    & 82.53\%                                                             & 81.91\%           \\ \hline
		SVM                                         & 93.24\%                                                                    & 94.11\%                                                             & 69.66\%            \\ \hline
		Random Forest                               & 91.65\%                                                                     & 88.95\%                                                             & 68.47\%             \\ \hline
		Adaboost                                    & 79.03\%                                                                    & 66.49\%                                                             & 60.98\%              \\ \hline
	\end{tabular}	
	\caption{Classification performance by algorithm for Eigenvector Centrality measure}

\end{table}

Therefore using the Eigenvector centrality measure does not give any remarkable improvement. The precisions score overall remain the same.
\subsection{Graph with Weighted Edges}
With the multigraph structure, we had a particularly small time of execution needed to compute the document term matrix which was of $120$s. Of course, we added every edge to the multigraph without checking if the edge already existed as it is done in the undirected graph structure we used previously. Since the eigenvector centrality measure did not seem to greatly improve our classification measure, we only tried this structure associated to a degree centrality measure.
\begin{table}[H]
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Algorithm \textbackslash Performance} & \begin{tabular}[c]{@{}c@{}}Micro-averaging\\ precision/recall\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ precision\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ recall\end{tabular} \\ \hline
		k-NN                                        & 86.25\%                                                                    & 86.96\%                                                             & 81.55\%           \\ \hline
		SVM                                         & 95.25\%                                                                    & 95.38\%                                                             & 79.62\%            \\ \hline
		Random Forest                               & 91.82\%                                                                     & 90.58\%                                                             & 69.33\%             \\ \hline
		Adaboost                                    & 79.21\%                                                                    & 69.01\%                                                             & 65.97\%              \\ \hline
	\end{tabular}
	\caption{Classification performance by algorithm for Weighted Edges with Degree Centrality measure}
\end{table}

Therefore the results obtained for a multigraph (ie undirected weighted graph) are slightly better. These improvement are not really significant but regarding to the time needed to compute the document term matrix which was divided by a factor $3$ we considered the multigraph as the best structure for our classification problem. The best classification algorithm still being the one-vs-one multiclass SVM like in our bag-of-words model.

\subsection{Influence of the size of the sliding window} 
As we previously said, we firstly made the choice to use a size of a sliding window of size $4$ to be sure to obtain good results. In a second time, we changed the size $k$ of the sliding window. Here is what we obtained with several values for $k$ for undirected weighted graphs with the degree centrality measure:

\begin{table}[H]
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Sliding window size\textbackslash Algorithm} & \begin{tabular}[c]{@{}c@{}}NN\end{tabular} & \begin{tabular}[c]{@{}c@{}}SVM\end{tabular} & \begin{tabular}[c]{@{}c@{}}Random Forest\end{tabular} \\ \hline
		$k=2$                                        & 82.46\%                                                                    & 93.24\%                                                           & 90.59\%           \\ \hline
		$k=3$                                        & 85.15\%                                                                    & 93.51\%                                                             & 89.22\%            \\ \hline
		$k=4$                               & 86.25\%                                                                     & 95.25\%                                                             & 91.82\%             \\ \hline
		$k=5$                                    & 86.57\%                                                                    & 93.56\%                                                             & 89.86\%              \\ \hline
	\end{tabular}
	\caption{Precision with micro-averaging by  algorithm for Degree Centrality measure}
\end{table}
 
As expected the best precision is obtained for our classification algorithms when the the size of the sliding window is of size $4$. Of course the time needed to construct those graphs increases with the size of the sliding window but in our problem this time was low enough to prefer a size of $4$ to a size of $3$ as we only $60s$ more to obtain results with an approximately $2\%$ better precision.

\section{Conclusion}
Regarding the time taken to construct the classification model, Random Forest is by far the best (150 times quicker than SVM). The classification performances are in the same order for all the features: first is SVM, then RF, then NN and finally Adaboost. Amongst all features, graph-of-words gives better results than bag-of-words, but takes more time to construct. For this model, degree centrality is better than eigenvector centrality. The influence of the side of the sliding window is critical, both in terms of precision and time complexity of the resulting graphs. A size of 4 is a good trade off between these two aspects. What's more, the use of a weighted edge graph improves by a factor of 3 the time taken to construct the document matrix.
\\From all of this, we recommend the use of RF with a graph-of-words model, with weighted graph and degree centrality. If recall is extra important and model can be constructed with all the time needed, then SVM can perform better. In other cases, it is quicker to train a RF.
\end{document}