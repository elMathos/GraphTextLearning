\documentclass[11pt,a4paper]{article}

\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[french]{babel}


\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-plot}
\usepackage{calc}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}  
\usepackage{lmodern}
\usepackage{stmaryrd}
\usepackage[]{algorithm2e}
\usepackage{float}

\pagestyle{plain}

\title{ADVANCED LEARNING FOR TEXT AND GRAPH DATA \\ Final Project: Text categorization}
\author{Mathurin \textsc{Massias} \and Clément \textsc{Nicolle} \and Michaël Weiss}
\date{\today} 


\begin{document}
	
\maketitle

\section{Bag-of-words model}

\subsection{Description and classification algorithms}

We first implemented the bag-of-words model. From the train and test datasets, two matrices of size n\_documents x n\_terms were computed, using the dictionary of terms present in the train dataset. Then, each document can be seen as a vector in dimension n\_terms.
\\We tried out several classification algorithm on it:
- k-Nearest Neighbors: for a given document of the test dataset, we find out the k closest documents in the train dataset using cosine similarity. Then, we assign to the test document the most frequent label among these k neighbors. Arbitrarily, in case of equality, the first label in alphabetic order among the most frequent will be picked up.
- Support Vector Machine: we used a Gaussian kernel, and cross-validation was made in order to select best values for parameters C and gamma.
- Random Forest: we made a cross-validation to select the optimal number of trees to compute.

\subsection{Results}

As suggested in the subject, we used micro-averaging and macro-averaging precision and recall in order to evaluate the performances of our classifiers. We can note that micro-averaging precision and recall are indeed the same number.



\end{document}