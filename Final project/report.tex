\documentclass[11pt,a4paper]{article}

\usepackage{latexsym}
\usepackage{graphicx}
\usepackage[french]{babel}


\usepackage{amsmath,amssymb}
\usepackage{pstricks,pst-plot}
\usepackage{calc}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}  
\usepackage{lmodern}
\usepackage{stmaryrd}
\usepackage[]{algorithm2e}
\usepackage{float}

\pagestyle{plain}

\title{ADVANCED LEARNING FOR TEXT AND GRAPH DATA \\ Final Project: Text categorization}
\author{Mathurin \textsc{Massias} \and Clément \textsc{Nicolle} \and Michaël Weiss}
\date{\today} 


\begin{document}
	
\maketitle

\section{Bag-of-words model}

\subsection{Description and classification algorithms}

We first implemented the bag-of-words model. From the train and test datasets, two matrices of size $n\_documents \times n\_terms$ were computed, using the dictionary of terms present in the train dataset. Then, each document is represented by a vector in dimension $n\_terms$.
\\We tried out several classification algorithms on the feature dataset:
\begin{itemize}
	\item k-Nearest Neighbors: for a given document of the test dataset, we find out the k closest documents in the train dataset using cosine similarity. Then, we assign to the test document the most frequent label among these k neighbors. Arbitrarily, in case of tie, the first label in alphabetic order among the most frequent will be picked up.
	\item Support Vector Machine: we used a Gaussian kernel, and cross-validation was made in order to select best values for parameters C and gamma.
	\item Random Forest: we made a cross-validation to select the optimal number of trees to compute.
	\item Adaboost: we also made a cross-validation for the number of boundaries here.
\end{itemize}


\subsection{Results}

As suggested in the subject, we used micro-averaging and macro-averaging precision and recall in order to evaluate the performances of our classifiers. We can notice that micro-averaging precision and recall are indeed the same number.
\\Cross-validations were made upstream in order to find the optimal values for the parameters: k for k-NN, C and $\gamma$ for RBF-SVM, number of trees for random forests. With these parameters fitted, here are the different performances the algorithms achieved :

\begin{table}[h]
\hspace*{-17mm}	\begin{tabular}{|l|c|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Algorithm \textbackslash Performance} & \begin{tabular}[c]{@{}c@{}}Micro-averaging\\ precision/recall\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ precision\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ recall\end{tabular} & Training time \\ \hline
		k-NN                                        & 84,42\%                                                                    & 85,40\%                                                             & 82,74\%                                                          & 1543 s           \\ \hline
		SVM                                         & 89,31\%                                                                    & 92,63\%                                                             & 65,06\%                                                          & 1021 s           \\ \hline
		Random Forest                               & 91,6\%                                                                     & 89,46\%                                                             & 62,84\%                                                          & 7 s           \\ \hline
		Adaboost                                    & 79,1\%                                                                    & 57,02\%                                                             & 57,86\%                                                          & 204 s           \\ \hline
	\end{tabular}
	\caption{Time and classification performance for various algorithms on the bag-of-words model}
	\end{table}

\section{Graph-of-words model}
The we implemented a Graph-of-words model. As the subject suggested it we firstly tried to implement it with windows of size $4$ to find the edges. This allowed us to have better classification results overall. \\
The two measure we associated to the graph nodes were degree centrality measure and eigenvector centrality measure.
The two models of graphs we used were undirected unweigthed graphs, and undirected unweighted multigraphs of the library networkx. The second one allows us to have multiple same edge between $2$ nodes and therefore to have an equivalent of an undirected weighted graph.\\
Since after the graphs were constructed, the final input to use for the classification algorithms remained a document-term matrix of the same size than the one obtained in the bag-of-words model, we applied the same classification algorithms in order to compare the results.
\subsection{Graphs with unweighted edges}
\paragraph{Degree Centrality measure \newline}
create data=312s, SVM=1395s, RF=1429453798s, kNN=149453794s, adaboost=1642s

\begin{table}[H]
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Algorithm \textbackslash Performance} & \begin{tabular}[c]{@{}c@{}}Micro-averaging\\ precision/recall\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ precision\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ recall\end{tabular} \\ \hline
		k-NN                                        & 86.16\%                                                                    & 83,64\%                                                             & 82,22\%           \\ \hline
		SVM                                         & 95,16\%                                                                    & 95,44\%                                                             & 79,89\%            \\ \hline
		Random Forest                               & 91,09\%                                                                     & 86,05\%                                                             & 66,07\%             \\ \hline
		Adaboost                                    & 79,53\%                                                                    & 67,18\%                                                             & 67,36\%              \\ \hline
	\end{tabular}
\end{table}
\subsection{Graph with Weighted Edges}
a peu pres la meme sauf timeStruc=134s
\paragraph{Degree Centrality measure \newline}
\begin{table}[H]
	\begin{tabular}{|l|c|c|c|}
		\hline
		\multicolumn{1}{|c|}{Algorithm \textbackslash Performance} & \begin{tabular}[c]{@{}c@{}}Micro-averaging\\ precision/recall\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ precision\end{tabular} & \begin{tabular}[c]{@{}c@{}}Macro-averaging\\ recall\end{tabular} \\ \hline
		k-NN                                        & 86,25\%                                                                    & 86,96\%                                                             & 81,55\%           \\ \hline
		SVM                                         & 95,25\%                                                                    & 95,38\%                                                             & 79,62\%            \\ \hline
		Random Forest                               & 91,82\%                                                                     & 90,58\%                                                             & 69,33\%             \\ \hline
		Adaboost                                    & 79,21\%                                                                    & 69,01\%                                                             & 65,97\%              \\ \hline
	\end{tabular}
\end{table}





\end{document}